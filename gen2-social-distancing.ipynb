{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Social distancing is one of the most known methods to avoid a contagious disease.\n",
    "\n",
    "In a world hit by a Pandemic, it is important to be careful and take precautions for our own as well as the greater good. Few ways to stay safe are Wearing a Mask and Keeping a Social Distance of 2 metres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Some viruses – like the virus that causes COVID-19 – spread easily, according to the Centers for Disease Control and Prevention. Social distancing puts space between individuals. If someone is sick and there are no people around, a virus cannot spread. \n",
    "While there are a lot of different available softwares/programs to detect social distancing in a crowded place, very few take into account all the 3 coordinates. This is where OAK-D comes into use, it can detect depth of an object i.e., giving the coordinates of the third axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "import time\n",
    "from itertools import combinations\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "\n",
    "We use the euclidean formula for distance calculation i.e., \n",
    "\n",
    "distance = sqrt(dx^2 + dy^2 + dz^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(dx, dy, dz):\n",
    "    distance = math.sqrt(dx ** 2 + dy ** 2 + dz ** 2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of TinyYoloV4 blob File\n",
    "\n",
    "We start with specifying the label map for Tiny Yolo V4. Though we can specify 80 labels as Tiny Yolo v4 can recognise 80 classes, we will just specify the first class which can be used to detect a person in an image and use the same for rest of the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a pipeline\n",
    "\n",
    "Before we go ahead, we need to define a pipeline. We also define various variables to define the different cameras available in OAK-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny yolo v3/4 label texts\n",
    "labelMap = [\n",
    "    \"person\"\n",
    "]\n",
    "\n",
    "syncNN = True\n",
    "\n",
    "# Start defining a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - color camera\n",
    "colorCam = pipeline.createColorCamera()\n",
    "spatialDetectionNetwork = pipeline.createYoloSpatialDetectionNetwork()\n",
    "monoLeft = pipeline.createMonoCamera()\n",
    "monoRight = pipeline.createMonoCamera()\n",
    "stereo = pipeline.createStereoDepth()\n",
    "\n",
    "xoutRgb = pipeline.createXLinkOut()\n",
    "xoutNN = pipeline.createXLinkOut()\n",
    "xoutBoundingBoxDepthMapping = pipeline.createXLinkOut()\n",
    "xoutDepth = pipeline.createXLinkOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transport results to host\n",
    "Next, we want to receive both color camera frames and neural network inference results - as these are produced on the device, they need to be transported to the host. The communication between device and host is handled by XLink. We then define the preview size and the different cameras of OAK-D as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xoutRgb.setStreamName(\"rgb\")\n",
    "xoutNN.setStreamName(\"detections\")\n",
    "xoutBoundingBoxDepthMapping.setStreamName(\"boundingBoxDepthMapping\")\n",
    "xoutDepth.setStreamName(\"depth\")\n",
    "\n",
    "# colorCam.setVideoSize(1000, 1000) # PREVIEW SIZE\n",
    "colorCam.setPreviewSize(416, 416)\n",
    "colorCam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "colorCam.setInterleaved(False)\n",
    "colorCam.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting node configs\n",
    "We then set the configs for node. This includes defining path of blog file, setting confidence threshold, setting boundinf box scale factor etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-354cdbcc6564>:1: DeprecationWarning: setOutputDepth() is deprecated, the output is auto-enabled if used.\n",
      "  stereo.setOutputDepth(True)\n"
     ]
    }
   ],
   "source": [
    "stereo.setOutputDepth(True)\n",
    "stereo.setConfidenceThreshold(255)\n",
    "\n",
    "spatialDetectionNetwork.setBlobPath('models/tiny-yolo-v4_openvino_2021.2_6shave.blob')\n",
    "spatialDetectionNetwork.setConfidenceThreshold(0.5)\n",
    "spatialDetectionNetwork.input.setBlocking(False)\n",
    "spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)\n",
    "spatialDetectionNetwork.setDepthLowerThreshold(100)\n",
    "spatialDetectionNetwork.setDepthUpperThreshold(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolo specific parameters\n",
    "We now need to specify the YOLO specific parameters which include number of classes which is 80 in our case, setting the coordinates size which is 4 as we have 4 coordinates, setting anchors, anchor masks and Input/Output Threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialDetectionNetwork.setNumClasses(80)\n",
    "spatialDetectionNetwork.setCoordinateSize(4)\n",
    "spatialDetectionNetwork.setAnchors(np.array([10,14, 23,27, 37,58, 81,82, 135,169, 344,319]))\n",
    "spatialDetectionNetwork.setAnchorMasks({ \"side26\": np.array([1,2,3]), \"side13\": np.array([3,4,5]) })\n",
    "spatialDetectionNetwork.setIouThreshold(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create outputs\n",
    "We create the outputs using all the parameters we defined above which includes the different cameras, the Yolo Model and Xlink Bounding box. This is also required to find the exact spacial location of an object detected by the YOLO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "monoLeft.out.link(stereo.left)\n",
    "monoRight.out.link(stereo.right)\n",
    "\n",
    "colorCam.preview.link(spatialDetectionNetwork.input)\n",
    "if syncNN:\n",
    "    spatialDetectionNetwork.passthrough.link(xoutRgb.input)\n",
    "else:\n",
    "    colorCam.preview.link(xoutRgb.input)\n",
    "\n",
    "spatialDetectionNetwork.out.link(xoutNN.input)\n",
    "spatialDetectionNetwork.boundingBoxMapping.link(xoutBoundingBoxDepthMapping.input)\n",
    "\n",
    "stereo.depth.link(spatialDetectionNetwork.inputDepth)\n",
    "spatialDetectionNetwork.passthroughDepth.link(xoutDepth.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Pipeline\n",
    "\n",
    "We now start the pipeline and connect it to a DepthAi device, which is OAK-D in our case.\n",
    "We then get the output detections using various variables as well as set a timer to find the frame count of OAK-D in real time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Distancing\n",
    "To detect social distancing, we first check if the number of detections are not zero, if there are detections, we next check if the detection is a Person or not. If the detection is a person, we set the boundaries of the bounding box using the YOLO model as well as find the coordinates of the detection on x, y and z axes using the spacial location of the detection in millimeters. \n",
    "\n",
    "Next, we check if there are 2 persons in a frame and if found, we check the distance between the detections, if this distance is less than 2 metres (2000 mm) then we add both the detections in a red-zone list and draw a red bounding box around them as well as a line showing the distance between the 2 detections. If the distance is greater than 2 metres then both the persons are in a safe zone and therefore marked as safe with a Green Bounding box. We then print the number of people at risk around the bottom of the screen as well as print the number of frames per seconds.\n",
    "\n",
    "Once all these is done, we display the output screen using the imshow() function in OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-220d43e729c6>:5: DeprecationWarning: Device(pipeline) starts the pipeline automatically. Use Device() and startPipeline(pipeline) otherwise\n",
      "  device.startPipeline()\n"
     ]
    }
   ],
   "source": [
    "# Pipeline is defined, now we can connect to the device\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "    # Start pipeline\n",
    "    device.startPipeline()\n",
    "\n",
    "    # Output queues will be used to get the rgb frames and nn data from the outputs defined above\n",
    "    previewQueue = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    detectionNNQueue = device.getOutputQueue(name=\"detections\", maxSize=4, blocking=False)\n",
    "    xoutBoundingBoxDepthMapping = device.getOutputQueue(name=\"boundingBoxDepthMapping\", maxSize=4, blocking=False)\n",
    "    depthQueue = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
    "\n",
    "    frame = None\n",
    "    detections = []\n",
    "\n",
    "    startTime = time.monotonic()\n",
    "    counter = 0\n",
    "    fps = 0\n",
    "    color = (255, 255, 255)\n",
    "\n",
    "    while True:\n",
    "        inPreview = previewQueue.get()\n",
    "        inNN = detectionNNQueue.get()\n",
    "        depth = depthQueue.get()\n",
    "\n",
    "        counter+=1\n",
    "        current_time = time.monotonic()\n",
    "        if (current_time - startTime) > 1 :\n",
    "            fps = counter / (current_time - startTime)\n",
    "            counter = 0\n",
    "            startTime = current_time\n",
    "\n",
    "        frame = inPreview.getCvFrame()\n",
    "        depthFrame = depth.getFrame()\n",
    "\n",
    "        depthFrameColor = cv2.normalize(depthFrame, None, 255, 0, cv2.NORM_INF, cv2.CV_8UC1)\n",
    "        depthFrameColor = cv2.equalizeHist(depthFrameColor)\n",
    "        depthFrameColor = cv2.applyColorMap(depthFrameColor, cv2.COLORMAP_HOT)\n",
    "        detections = inNN.detections\n",
    "\n",
    "        if len(detections) != 0:\n",
    "\n",
    "        # If the frame is available, draw bounding boxes on it and show the frame\n",
    "            height = frame.shape[0]\n",
    "            width  = frame.shape[1]\n",
    "            centroid_dict = dict()\n",
    "            objectId = 0\n",
    "            for detection in detections:\n",
    "                try:\n",
    "                    label = labelMap[detection.label]\n",
    "                except:\n",
    "                    label = detection.label\n",
    "                if str(label) == 'person' :\n",
    "                    boundingBoxMapping = xoutBoundingBoxDepthMapping.get()\n",
    "                    roiDatas = boundingBoxMapping.getConfigData()\n",
    "\n",
    "                    for roiData in roiDatas:\n",
    "                        roi = roiData.roi\n",
    "                        roi = roi.denormalize(depthFrameColor.shape[1], depthFrameColor.shape[0])\n",
    "                        topLeft = roi.topLeft()\n",
    "                        bottomRight = roi.bottomRight()\n",
    "                        xmin = int(topLeft.x)\n",
    "                        ymin = int(topLeft.y)\n",
    "                        xmax = int(bottomRight.x)\n",
    "                        ymax = int(bottomRight.y)\n",
    "\n",
    "                    # Denormalize bounding box\n",
    "                        x1 = int(detection.xmin * width)\n",
    "                        x2 = int(detection.xmax * width)\n",
    "                        y1 = int(detection.ymin * height)\n",
    "                        y2 = int(detection.ymax * height)\n",
    "                        center = (int((x1+x2)/2), int((y1+y2)/2))\n",
    "                        xsp, ysp, zsp = int(detection.spatialCoordinates.x), int(detection.spatialCoordinates.y), int(detection.spatialCoordinates.z)\n",
    "                        centroid_dict[objectId] = (x1, x2, y1, y2, xsp, ysp, zsp, center)\n",
    "                        objectId += 1\n",
    "\n",
    "            red_zone_list = [] # List containing which Object id is in under threshold distance condition.\n",
    "\n",
    "            for (id1, p1), (id2, p2) in combinations(centroid_dict.items(), 2):\n",
    "                dx, dy, dz = p1[4] - p2[4], p1[5] - p2[5], p1[6] - p2[6]\n",
    "                distance = calculate_distance(dx, dy, dz)\n",
    "                if(int(distance) < 2000 and int(distance) != 0):\n",
    "                    start_point=p1[7]\n",
    "                    end_point=p2[7]\n",
    "                    text_coord = (int((start_point[0]+end_point[0])/2),int((start_point[1]+end_point[1])/2)+20)\n",
    "                    cv2.line(frame, start_point, end_point, (0, 0, 255), 2)\n",
    "                    cv2.putText(frame, str(round(distance/1000,2))+' m', text_coord, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "                    if id1 not in red_zone_list:\n",
    "                        red_zone_list.append(id1)\n",
    "                    if id2 not in red_zone_list:\n",
    "                        red_zone_list.append(id2)\n",
    "\n",
    "            for idx, box in centroid_dict.items():  \n",
    "                if idx in red_zone_list:   # if id is in red zone list\n",
    "                    cv2.rectangle(frame, (box[0], box[2]), (box[1], box[3]), (0, 0, 255), 2) # Create Red bounding boxes  #starting point, ending point size of 2\n",
    "                else:\n",
    "                    cv2.rectangle(frame, (box[0], box[2]), (box[1], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "            text = \"No of at-risk people: %s\" % str(int(len(red_zone_list)/2))           # Count People at Risk\n",
    "            location = (10,25)                          # Set the location of the displayed text\n",
    "            cv2.putText(frame, text, location, cv2.FONT_HERSHEY_SIMPLEX, 1, (246,86,86), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.putText(frame, \"NN fps: {:.2f}\".format(fps), (2, frame.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color)\n",
    "        cv2.imshow(\"rgb\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
